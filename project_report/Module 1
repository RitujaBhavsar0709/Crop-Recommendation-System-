The main goal of Module 1 is to collect, clean, and prepare the dataset for machine learning model training.
It ensures that the input data (soil and climate parameters) is accurate, complete, and ready for analysis.

Submodules and Tasks
1.1 Data Collection

Source:
The dataset can be obtained from Kaggle (Crop Recommendation Dataset).
Dataset Link

Data Format: CSV file (Crop_recommendation.csv)

Features in Dataset:

Feature	Description
N	Nitrogen content in soil
P	Phosphorus content in soil
K	Potassium content in soil
temperature	Ambient temperature (°C)
humidity	Humidity (%)
ph	Soil pH level
rainfall	Average rainfall (mm)
label	Crop name (Target variable)
1.2 Data Loading

Using Python (Pandas) to load the dataset.

import pandas as pd

# Load dataset
data = pd.read_csv("Crop_recommendation.csv")

# Display first few rows
print(data.head())

1.3 Data Exploration (EDA)

Explore the dataset to understand its structure and properties.

# Basic info
print(data.info())

# Summary statistics
print(data.describe())

# Unique crops
print("Number of Crops:", len(data['label'].unique()))
print("Crops List:", data['label'].unique())


You can also visualize the data using Matplotlib or Seaborn.

import seaborn as sns
import matplotlib.pyplot as plt

# Plot correlation heatmap
plt.figure(figsize=(10,6))
sns.heatmap(data.corr(), annot=True, cmap='Greens')
plt.title("Feature Correlation Heatmap")
plt.show()

1.4 Data Cleaning

Check for missing values or invalid data.

# Check for missing/null values
print(data.isnull().sum())

# Remove duplicates if any
data.drop_duplicates(inplace=True)


If any missing data exists, you can fill it:

data.fillna(method='ffill', inplace=True)

1.5 Feature Selection

Separate input and output columns.

X = data.drop('label', axis=1)  # Independent variables
y = data['label']               # Target variable

1.6 Data Normalization / Scaling

Normalize the data to ensure consistent scale across features.

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

1.7 Train-Test Split

Divide the dataset into training and testing sets.

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)

 Output of Module 1

After completing this module, you’ll have:

 Cleaned and processed dataset
 Features and target variable separated
 Scaled data ready for model training
 Training and testing sets created
